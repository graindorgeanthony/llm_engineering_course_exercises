{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E8OVY8PFMCtW"
      },
      "source": [
        "# Fine-Tuning a Model + Upload on Hugging Face\n",
        "\n",
        "This notebook:\n",
        "1. Loads the base Llama 3.2-3B model with QLoRA (Quantized LoRA)\n",
        "2. Loads the training dataset from Hugging Face\n",
        "3. Configures LoRA parameters for efficient fine-tuning\n",
        "4. Trains the model using Supervised Fine-Tuning (SFT)\n",
        "5. Uploads the fine-tuned model to Hugging Face Hub\n",
        "\n",
        "QLoRA combines quantization (4-bit) with LoRA adapters, allowing efficient\n",
        "fine-tuning of large models on consumer hardware."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-9yUYbuQMCtY",
        "outputId": "1e5707da-f40b-4941-d19f-b5e3bbd3efd9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.4/59.4 MB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m465.5/465.5 kB\u001b[0m \u001b[31m21.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q --upgrade bitsandbytes==0.48.2 trl==0.25.1\n",
        "!wget -q https://raw.githubusercontent.com/ed-donner/llm_engineering/main/week7/util.py -O util.py"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "xMD-SnFjMCtZ"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "from huggingface_hub import login\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import (\n",
        "    AutoModelForCausalLM,\n",
        "    AutoTokenizer,\n",
        "    BitsAndBytesConfig,\n",
        "    set_seed\n",
        ")\n",
        "from datasets import load_dataset\n",
        "from peft import LoraConfig\n",
        "from trl import SFTTrainer, SFTConfig\n",
        "from datetime import datetime\n",
        "import sys\n",
        "from pathlib import Path\n",
        "\n",
        "# Optional: Weights & Biases for experiment tracking\n",
        "try:\n",
        "    import wandb\n",
        "    WANDB_AVAILABLE = True\n",
        "except ImportError:\n",
        "    WANDB_AVAILABLE = False\n",
        "    print(\"Warning: wandb not available. Set LOG_TO_WANDB=False or install with: pip install wandb\")\n",
        "\n",
        "# ============================================================================\n",
        "# CONFIGURATION CONSTANTS\n",
        "# ============================================================================\n",
        "\n",
        "# Model Configuration\n",
        "BASE_MODEL = \"meta-llama/Llama-3.2-3B\"  # Base model to fine-tune\n",
        "PROJECT_NAME = \"price\"  # Project name for organizing runs\n",
        "HF_USER = \"Anthonygdg123\"  # Your Hugging Face username\n",
        "\n",
        "# Dataset Configuration\n",
        "LITE_MODE = True  # True: use lite dataset (faster, less data), False: use full dataset\n",
        "DATA_USER = \"Anthonygdg123\"  # Hugging Face username for dataset\n",
        "\n",
        "# Run Configuration\n",
        "RUN_NAME = f\"{datetime.now():%Y-%m-%d_%H.%M.%S}\"  # Unique run identifier with timestamp\n",
        "if LITE_MODE:\n",
        "    RUN_NAME += \"-lite\"  # Append \"-lite\" suffix for lite mode runs\n",
        "PROJECT_RUN_NAME = f\"{PROJECT_NAME}-{RUN_NAME}\"  # Full project run name\n",
        "HUB_MODEL_NAME = f\"{HF_USER}/{PROJECT_RUN_NAME}\"  # Hugging Face Hub model identifier\n",
        "\n",
        "# ============================================================================\n",
        "# HYPERPARAMETERS - Overall Training\n",
        "# ============================================================================\n",
        "\n",
        "EPOCHS = 1 if LITE_MODE else 3  # Number of complete passes through the training dataset\n",
        "BATCH_SIZE = 32 if LITE_MODE else 256  # Number of examples processed per batch (larger = faster but more memory)\n",
        "MAX_SEQUENCE_LENGTH = 128  # Maximum number of tokens in input sequences (truncates longer sequences)\n",
        "GRADIENT_ACCUMULATION_STEPS = 1  # Number of batches to accumulate before updating weights (simulates larger batch size)\n",
        "\n",
        "# ============================================================================\n",
        "# HYPERPARAMETERS - QLoRA (Quantized LoRA)\n",
        "# ============================================================================\n",
        "\n",
        "QUANT_4_BIT = True  # Use 4-bit quantization (True) or 8-bit (False) - 4-bit saves more memory\n",
        "LORA_R = 32 if LITE_MODE else 256  # Rank of LoRA matrices (higher = more parameters, more capacity but slower)\n",
        "LORA_ALPHA = LORA_R * 2  # LoRA alpha scaling factor (controls adapter strength, typically 2x rank)\n",
        "ATTENTION_LAYERS = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]  # Attention layers to apply LoRA adapters\n",
        "MLP_LAYERS = [\"gate_proj\", \"up_proj\", \"down_proj\"]  # MLP layers to apply LoRA adapters\n",
        "TARGET_MODULES = ATTENTION_LAYERS if LITE_MODE else ATTENTION_LAYERS + MLP_LAYERS  # Which layers get LoRA adapters\n",
        "LORA_DROPOUT = 0.1  # Dropout rate for LoRA adapters (prevents overfitting, 0.1 = 10% dropout)\n",
        "\n",
        "# ============================================================================\n",
        "# HYPERPARAMETERS - Training\n",
        "# ============================================================================\n",
        "\n",
        "LEARNING_RATE = 1e-4  # Step size for weight updates (0.0001, higher = faster learning but less stable)\n",
        "WARMUP_RATIO = 0.01  # Fraction of training steps for learning rate warmup (gradual increase from 0)\n",
        "LR_SCHEDULER_TYPE = 'cosine'  # Learning rate schedule (cosine = smooth decrease, linear = constant decrease)\n",
        "WEIGHT_DECAY = 0.001  # L2 regularization strength (penalizes large weights to prevent overfitting)\n",
        "OPTIMIZER = \"paged_adamw_32bit\"  # Optimizer algorithm (paged_adamw_32bit = memory-efficient AdamW)\n",
        "\n",
        "# ============================================================================\n",
        "# HYPERPARAMETERS - Tracking & Logging\n",
        "# ============================================================================\n",
        "\n",
        "VAL_SIZE = 500 if LITE_MODE else 1000  # Number of validation examples to use for evaluation\n",
        "LOG_STEPS = 5 if LITE_MODE else 10  # Frequency of logging metrics (every N steps)\n",
        "SAVE_STEPS = 100 if LITE_MODE else 200  # Frequency of saving checkpoints (every N steps)\n",
        "LOG_TO_WANDB = True  # Enable Weights & Biases logging for experiment tracking"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "8t9M69CoMCtZ"
      },
      "outputs": [],
      "source": [
        "def load_environment():\n",
        "    \"\"\"\n",
        "    Load environment variables and authenticate with Hugging Face and W&B.\n",
        "\n",
        "    Requires HF_TOKEN environment variable to be set.\n",
        "    Optionally requires WANDB_API_KEY if LOG_TO_WANDB is True.\n",
        "    \"\"\"\n",
        "\n",
        "    # Authenticate with Hugging Face\n",
        "    hf_token = userdata.get('HF_TOKEN')\n",
        "    if not hf_token:\n",
        "        raise ValueError(\n",
        "            \"HF_TOKEN not found. Please set it as an environment variable or in a .env file.\\n\"\n",
        "            \"You can get a token from: https://huggingface.co/settings/tokens\"\n",
        "        )\n",
        "\n",
        "    login(hf_token, add_to_git_credential=True)\n",
        "    print(\"✓ Successfully logged in to Hugging Face\")\n",
        "\n",
        "    # Authenticate with Weights & Biases if enabled\n",
        "    if LOG_TO_WANDB:\n",
        "        if not WANDB_AVAILABLE:\n",
        "            print(\"Warning: wandb not available. Disabling W&B logging.\")\n",
        "            return hf_token\n",
        "\n",
        "        wandb_api_key = userdata.get('WANDB_API_KEY')\n",
        "        if wandb_api_key:\n",
        "            os.environ[\"WANDB_API_KEY\"] = wandb_api_key\n",
        "            wandb.login()\n",
        "            os.environ[\"WANDB_PROJECT\"] = PROJECT_NAME\n",
        "            os.environ[\"WANDB_LOG_MODEL\"] = \"false\"  # Don't log full model to W&B (too large)\n",
        "            os.environ[\"WANDB_WATCH\"] = \"false\"  # Don't watch gradients (reduces overhead)\n",
        "            print(\"✓ Successfully logged in to Weights & Biases\")\n",
        "        else:\n",
        "            print(\"Warning: WANDB_API_KEY not found. Disabling W&B logging.\")\n",
        "\n",
        "    return hf_token\n",
        "\n",
        "\n",
        "def load_dataset_from_hub(lite_mode: bool = True, username: str = \"ed-donner\", val_size: int = 500):\n",
        "    \"\"\"\n",
        "    Load the prompts dataset from Hugging Face Hub.\n",
        "\n",
        "    Args:\n",
        "        lite_mode: If True, loads items_prompts_lite, else loads items_prompts_full\n",
        "        username: Hugging Face username for the dataset\n",
        "        val_size: Number of validation examples to use\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (train, val, test) datasets\n",
        "    \"\"\"\n",
        "    # Determine which dataset to load\n",
        "    dataset_name = f\"{username}/items_prompts_lite\" if lite_mode else f\"{username}/items_prompts_full\"\n",
        "\n",
        "    print(f\"Loading dataset: {dataset_name}\")\n",
        "\n",
        "    # Load dataset from Hugging Face Hub\n",
        "    dataset = load_dataset(dataset_name)\n",
        "    train = dataset['train']\n",
        "    val = dataset['val'].select(range(val_size))  # Select subset for faster evaluation\n",
        "    test = dataset['test']\n",
        "\n",
        "    print(f\"✓ Loaded {len(train):,} training examples\")\n",
        "    print(f\"✓ Loaded {len(val):,} validation examples\")\n",
        "    print(f\"✓ Loaded {len(test):,} test examples\")\n",
        "\n",
        "    return train, val, test\n",
        "\n",
        "\n",
        "def detect_compute_dtype():\n",
        "    \"\"\"\n",
        "    Detect the best compute dtype based on GPU capabilities.\n",
        "\n",
        "    bfloat16 is preferred for A100/H100 GPUs (compute capability >= 8.0)\n",
        "    float16 is used for older GPUs (T4, V100, etc.)\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (use_bf16 boolean, compute_dtype)\n",
        "    \"\"\"\n",
        "    if torch.cuda.is_available():\n",
        "        capability = torch.cuda.get_device_capability()\n",
        "        use_bf16 = capability[0] >= 8  # A100/H100 and newer\n",
        "        compute_dtype = torch.bfloat16 if use_bf16 else torch.float16\n",
        "        print(f\"GPU capability: {capability}, using {'bfloat16' if use_bf16 else 'float16'}\")\n",
        "    else:\n",
        "        use_bf16 = False\n",
        "        compute_dtype = torch.float16\n",
        "        print(\"CUDA not available, using float16\")\n",
        "\n",
        "    return use_bf16, compute_dtype\n",
        "\n",
        "\n",
        "def create_quantization_config(use_4bit: bool = True, compute_dtype=torch.bfloat16):\n",
        "    \"\"\"\n",
        "    Create quantization configuration for QLoRA.\n",
        "\n",
        "    QLoRA uses 4-bit quantization to reduce memory usage while maintaining\n",
        "    model quality through careful quantization techniques.\n",
        "\n",
        "    Args:\n",
        "        use_4bit: If True, use 4-bit quantization, else use 8-bit\n",
        "        compute_dtype: Data type for computations (bfloat16 or float16)\n",
        "\n",
        "    Returns:\n",
        "        BitsAndBytesConfig object\n",
        "    \"\"\"\n",
        "    if use_4bit:\n",
        "        print(\"Using 4-bit quantization (NF4 with double quantization)\")\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_4bit=True,  # Enable 4-bit quantization\n",
        "            bnb_4bit_use_double_quant=True,  # Double quantization for better compression\n",
        "            bnb_4bit_compute_dtype=compute_dtype,  # Compute dtype for 4-bit base\n",
        "            bnb_4bit_quant_type=\"nf4\"  # Normal Float 4-bit quantization (better than standard 4-bit)\n",
        "        )\n",
        "    else:\n",
        "        print(\"Using 8-bit quantization\")\n",
        "        quant_config = BitsAndBytesConfig(\n",
        "            load_in_8bit=True,\n",
        "            bnb_8bit_compute_dtype=compute_dtype\n",
        "        )\n",
        "\n",
        "    return quant_config\n",
        "\n",
        "\n",
        "def load_base_model(base_model_name: str, quant_config: BitsAndBytesConfig):\n",
        "    \"\"\"\n",
        "    Load the base model with quantization and configure tokenizer.\n",
        "\n",
        "    Args:\n",
        "        base_model_name: Name of the model on Hugging Face Hub\n",
        "        quant_config: Quantization configuration\n",
        "\n",
        "    Returns:\n",
        "        Tuple of (tokenizer, model)\n",
        "    \"\"\"\n",
        "    print(f\"\\nLoading tokenizer for {base_model_name}...\")\n",
        "\n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(base_model_name, trust_remote_code=True)\n",
        "\n",
        "    # Set padding token (required for batch processing)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "    # Set padding side to right (standard for causal LMs)\n",
        "    tokenizer.padding_side = \"right\"\n",
        "\n",
        "    print(\"✓ Tokenizer loaded\")\n",
        "\n",
        "    print(f\"\\nLoading model {base_model_name} with quantization...\")\n",
        "    print(\"This may take a few minutes...\")\n",
        "\n",
        "    # Load model with quantization\n",
        "    base_model = AutoModelForCausalLM.from_pretrained(\n",
        "        base_model_name,\n",
        "        quantization_config=quant_config,\n",
        "        device_map=\"auto\",  # Automatically distribute model across available GPUs/CPU\n",
        "    )\n",
        "\n",
        "    # Set pad token ID in generation config\n",
        "    base_model.generation_config.pad_token_id = tokenizer.pad_token_id\n",
        "\n",
        "    # Print memory footprint\n",
        "    memory_mb = base_model.get_memory_footprint() / 1e6\n",
        "    print(f\"✓ Model loaded\")\n",
        "    print(f\"  Memory footprint: {memory_mb:.1f} MB\")\n",
        "\n",
        "    return tokenizer, base_model\n",
        "\n",
        "\n",
        "def create_lora_config(\n",
        "    r: int = 32,\n",
        "    alpha: int = 64,\n",
        "    dropout: float = 0.1,\n",
        "    target_modules: list = None,\n",
        "    task_type: str = \"CAUSAL_LM\"\n",
        "):\n",
        "    \"\"\"\n",
        "    Create LoRA (Low-Rank Adaptation) configuration.\n",
        "\n",
        "    LoRA adds trainable low-rank matrices to specific layers, allowing\n",
        "    efficient fine-tuning with minimal additional parameters.\n",
        "\n",
        "    Args:\n",
        "        r: Rank of LoRA matrices (lower = fewer parameters, faster training)\n",
        "        alpha: LoRA alpha scaling factor (typically 2x rank, controls adapter strength)\n",
        "        dropout: Dropout rate for LoRA adapters (prevents overfitting)\n",
        "        target_modules: List of layer names to apply LoRA adapters\n",
        "        task_type: Type of task (CAUSAL_LM for language modeling)\n",
        "\n",
        "    Returns:\n",
        "        LoraConfig object\n",
        "    \"\"\"\n",
        "    if target_modules is None:\n",
        "        target_modules = [\"q_proj\", \"v_proj\", \"k_proj\", \"o_proj\"]\n",
        "\n",
        "    print(f\"\\nConfiguring LoRA:\")\n",
        "    print(f\"  Rank (r): {r}\")\n",
        "    print(f\"  Alpha: {alpha}\")\n",
        "    print(f\"  Dropout: {dropout}\")\n",
        "    print(f\"  Target modules: {target_modules}\")\n",
        "\n",
        "    lora_config = LoraConfig(\n",
        "        lora_alpha=alpha,  # Scaling factor for LoRA weights\n",
        "        lora_dropout=dropout,  # Dropout probability for LoRA layers\n",
        "        r=r,  # Rank of LoRA matrices (number of trainable parameters)\n",
        "        bias=\"none\",  # Don't train bias terms (none = no bias, all = all biases, lora_only = only LoRA biases)\n",
        "        task_type=task_type,  # Task type for PEFT\n",
        "        target_modules=target_modules,  # Which layers to apply LoRA adapters\n",
        "    )\n",
        "\n",
        "    return lora_config\n",
        "\n",
        "\n",
        "def create_training_config(\n",
        "    output_dir: str,\n",
        "    num_epochs: int,\n",
        "    batch_size: int,\n",
        "    learning_rate: float,\n",
        "    warmup_ratio: float,\n",
        "    lr_scheduler_type: str,\n",
        "    weight_decay: float,\n",
        "    optimizer: str,\n",
        "    max_sequence_length: int,\n",
        "    gradient_accumulation_steps: int,\n",
        "    save_steps: int,\n",
        "    logging_steps: int,\n",
        "    use_bf16: bool,\n",
        "    hub_model_id: str,\n",
        "    run_name: str,\n",
        "    log_to_wandb: bool = False\n",
        "):\n",
        "    \"\"\"\n",
        "    Create Supervised Fine-Tuning (SFT) configuration.\n",
        "\n",
        "    SFTConfig contains all training hyperparameters and settings for the\n",
        "    SFTTrainer, which handles the fine-tuning process.\n",
        "\n",
        "    Args:\n",
        "        output_dir: Directory to save checkpoints\n",
        "        num_epochs: Number of training epochs\n",
        "        batch_size: Batch size per device\n",
        "        learning_rate: Initial learning rate\n",
        "        warmup_ratio: Fraction of steps for warmup\n",
        "        lr_scheduler_type: Learning rate scheduler type\n",
        "        weight_decay: Weight decay for regularization\n",
        "        optimizer: Optimizer name\n",
        "        max_sequence_length: Maximum sequence length\n",
        "        gradient_accumulation_steps: Steps to accumulate gradients\n",
        "        save_steps: Frequency of saving checkpoints\n",
        "        logging_steps: Frequency of logging metrics\n",
        "        use_bf16: Whether to use bfloat16 precision\n",
        "        hub_model_id: Hugging Face Hub model ID\n",
        "        run_name: Name for this training run\n",
        "        log_to_wandb: Whether to log to Weights & Biases\n",
        "\n",
        "    Returns:\n",
        "        SFTConfig object\n",
        "    \"\"\"\n",
        "    print(f\"\\nConfiguring training:\")\n",
        "    print(f\"  Epochs: {num_epochs}\")\n",
        "    print(f\"  Batch size: {batch_size}\")\n",
        "    print(f\"  Learning rate: {learning_rate}\")\n",
        "    print(f\"  Max sequence length: {max_sequence_length}\")\n",
        "    print(f\"  Precision: {'bfloat16' if use_bf16 else 'float16'}\")\n",
        "\n",
        "    train_config = SFTConfig(\n",
        "        # Output and saving\n",
        "        output_dir=output_dir,  # Directory to save model checkpoints\n",
        "        save_steps=save_steps,  # Save checkpoint every N steps\n",
        "        save_total_limit=10,  # Maximum number of checkpoints to keep (deletes oldest)\n",
        "        save_strategy=\"steps\",  # Save based on steps (alternative: \"epoch\")\n",
        "\n",
        "        # Training parameters\n",
        "        num_train_epochs=num_epochs,  # Number of complete passes through training data\n",
        "        per_device_train_batch_size=batch_size,  # Batch size per GPU/device\n",
        "        per_device_eval_batch_size=1,  # Batch size for evaluation (smaller for memory efficiency)\n",
        "        gradient_accumulation_steps=gradient_accumulation_steps,  # Accumulate gradients over N batches\n",
        "        max_steps=-1,  # Maximum training steps (-1 = use epochs instead)\n",
        "\n",
        "        # Optimization\n",
        "        learning_rate=learning_rate,  # Initial learning rate\n",
        "        warmup_ratio=warmup_ratio,  # Fraction of training for warmup (gradual LR increase)\n",
        "        lr_scheduler_type=lr_scheduler_type,  # Learning rate schedule (cosine, linear, etc.)\n",
        "        weight_decay=weight_decay,  # L2 regularization strength\n",
        "        optim=optimizer,  # Optimizer algorithm (paged_adamw_32bit = memory-efficient)\n",
        "        max_grad_norm=0.3,  # Gradient clipping threshold (prevents exploding gradients)\n",
        "\n",
        "        # Precision and performance\n",
        "        fp16=not use_bf16,  # Use float16 precision (if not using bfloat16)\n",
        "        bf16=use_bf16,  # Use bfloat16 precision (better for A100/H100 GPUs)\n",
        "        group_by_length=True,  # Group similar-length sequences together (faster training)\n",
        "\n",
        "        # Sequence handling\n",
        "        max_length=max_sequence_length,  # Maximum sequence length (truncates longer sequences)\n",
        "\n",
        "        # Evaluation\n",
        "        eval_strategy=\"steps\",  # Evaluate based on steps (alternative: \"epoch\", \"no\")\n",
        "        eval_steps=save_steps,  # Evaluate every N steps (same as save_steps)\n",
        "\n",
        "        # Logging\n",
        "        logging_steps=logging_steps,  # Log metrics every N steps\n",
        "        report_to=\"wandb\" if log_to_wandb else None,  # Logging backend (wandb, tensorboard, None)\n",
        "        run_name=run_name,  # Name for this training run\n",
        "\n",
        "        # Hugging Face Hub\n",
        "        hub_strategy=\"every_save\",  # Push to hub every time we save (alternative: \"checkpoint\", \"end\")\n",
        "        push_to_hub=True,  # Automatically push model to Hugging Face Hub\n",
        "        hub_model_id=hub_model_id,  # Model identifier on Hugging Face Hub\n",
        "        hub_private_repo=True,  # Make repository private (False = public)\n",
        "    )\n",
        "\n",
        "    return train_config"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "2e4f37af0f3640cbac6c9f2a23971aca",
            "11cd72dab74e4fe1b1ff4b86012f9226",
            "77029cbffa9245df874a385becd30415",
            "ceb8c838fea54b1987ddc0a74999ffcd",
            "26495c9de0d448fba45034c2438d1305",
            "6ee55e20253442ba80f42e33be413df8",
            "5a9e9a4ba75541ccb4529c0ee0802515",
            "01f38dabdc4b4735b729b2584ab01b9b",
            "02507b8dee1d4437aa10f398711892cf",
            "55a3029adaa0470a8d1e17d21fb6bd66",
            "5756ba5d09714ffeaa284420f4e963c1"
          ]
        },
        "id": "ZgSvgvSZMCta",
        "outputId": "f6d51b64-5704-4dba-b101-331950761de9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "============================================================\n",
            "Fine-Tuning Model with QLoRA\n",
            "============================================================\n",
            "Model: meta-llama/Llama-3.2-3B\n",
            "Mode: LITE\n",
            "Run name: 2026-01-28_06.12.23-lite\n",
            "Hub model: Anthonygdg123/price-2026-01-28_06.12.23-lite\n",
            "============================================================\n",
            "✓ Successfully logged in to Hugging Face\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Calling wandb.login() after wandb.init() has no effect.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Successfully logged in to Weights & Biases\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Finishing previous runs because reinit is set to 'default'."
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/entropy</td><td>█▁</td></tr><tr><td>train/epoch</td><td>▁█</td></tr><tr><td>train/global_step</td><td>▁█</td></tr><tr><td>train/grad_norm</td><td>█▁</td></tr><tr><td>train/learning_rate</td><td>▁█</td></tr><tr><td>train/loss</td><td>█▁</td></tr><tr><td>train/mean_token_accuracy</td><td>▁█</td></tr><tr><td>train/num_tokens</td><td>▁█</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>train/entropy</td><td>2.87726</td></tr><tr><td>train/epoch</td><td>0.016</td></tr><tr><td>train/global_step</td><td>10</td></tr><tr><td>train/grad_norm</td><td>1.59077</td></tr><tr><td>train/learning_rate</td><td>0.0001</td></tr><tr><td>train/loss</td><td>1.7365</td></tr><tr><td>train/mean_token_accuracy</td><td>0.71875</td></tr><tr><td>train/num_tokens</td><td>39026</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">2026-01-28_06.04.59-lite</strong> at: <a href='https://wandb.ai/okuden/price/runs/vpqhny2m' target=\"_blank\">https://wandb.ai/okuden/price/runs/vpqhny2m</a><br> View project at: <a href='https://wandb.ai/okuden/price' target=\"_blank\">https://wandb.ai/okuden/price</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20260128_060515-vpqhny2m/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.24.0"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20260128_061233-qrb31fk4</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/okuden/price/runs/qrb31fk4' target=\"_blank\">2026-01-28_06.12.23-lite</a></strong> to <a href='https://wandb.ai/okuden/price' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/okuden/price' target=\"_blank\">https://wandb.ai/okuden/price</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/okuden/price/runs/qrb31fk4' target=\"_blank\">https://wandb.ai/okuden/price/runs/qrb31fk4</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Weights & Biases initialized\n",
            "Loading dataset: Anthonygdg123/items_prompts_lite\n",
            "✓ Loaded 20,000 training examples\n",
            "✓ Loaded 500 validation examples\n",
            "✓ Loaded 1,000 test examples\n",
            "GPU capability: (7, 5), using float16\n",
            "Using 4-bit quantization (NF4 with double quantization)\n",
            "\n",
            "Loading tokenizer for meta-llama/Llama-3.2-3B...\n",
            "✓ Tokenizer loaded\n",
            "\n",
            "Loading model meta-llama/Llama-3.2-3B with quantization...\n",
            "This may take a few minutes...\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2e4f37af0f3640cbac6c9f2a23971aca",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model loaded\n",
            "  Memory footprint: 2197.6 MB\n",
            "\n",
            "Configuring LoRA:\n",
            "  Rank (r): 32\n",
            "  Alpha: 64\n",
            "  Dropout: 0.1\n",
            "  Target modules: ['q_proj', 'v_proj', 'k_proj', 'o_proj']\n",
            "\n",
            "Configuring training:\n",
            "  Epochs: 1\n",
            "  Batch size: 32\n",
            "  Learning rate: 0.0001\n",
            "  Max sequence length: 128\n",
            "  Precision: float16\n",
            "\n",
            "Creating SFTTrainer...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The tokenizer has new PAD/BOS/EOS tokens that differ from the model config and generation config. The model config and generation config were aligned accordingly, being updated with the tokenizer's values. Updated tokens: {'pad_token_id': None}.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Trainer created\n",
            "\n",
            "============================================================\n",
            "Starting Training\n",
            "============================================================\n",
            "This may take a while depending on your hardware and dataset size...\n",
            "The model will be automatically saved to Hugging Face Hub during training.\n",
            "\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='293' max='625' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [293/625 27:09 < 30:58, 0.18 it/s, Epoch 0.47/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>Entropy</th>\n",
              "      <th>Num Tokens</th>\n",
              "      <th>Mean Token Accuracy</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>1.304200</td>\n",
              "      <td>1.290649</td>\n",
              "      <td>2.582803</td>\n",
              "      <td>330223.000000</td>\n",
              "      <td>0.757500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>1.266000</td>\n",
              "      <td>1.269200</td>\n",
              "      <td>2.695099</td>\n",
              "      <td>659017.000000</td>\n",
              "      <td>0.759500</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n",
            "/usr/local/lib/python3.12/dist-packages/torch/_dynamo/eval_frame.py:1044: UserWarning: torch.utils.checkpoint: the use_reentrant parameter should be passed explicitly. Starting in PyTorch 2.9, calling checkpoint without use_reentrant will raise an exception. use_reentrant=False is recommended, but if you need to preserve the current default behavior, you can pass use_reentrant=True. Refer to docs for more details on the differences between the two variants.\n",
            "  return fn(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "def create_trainer(model, tokenizer, train_dataset, eval_dataset, lora_config, train_config):\n",
        "    \"\"\"\n",
        "    Create SFTTrainer for supervised fine-tuning.\n",
        "\n",
        "    SFTTrainer handles the training loop, applying LoRA adapters and\n",
        "    managing the fine-tuning process.\n",
        "\n",
        "    Args:\n",
        "        model: Base model with quantization\n",
        "        tokenizer: Tokenizer instance\n",
        "        train_dataset: Training dataset\n",
        "        eval_dataset: Validation dataset\n",
        "        lora_config: LoRA configuration\n",
        "        train_config: Training configuration\n",
        "\n",
        "    Returns:\n",
        "        SFTTrainer object\n",
        "    \"\"\"\n",
        "    print(\"\\nCreating SFTTrainer...\")\n",
        "\n",
        "    trainer = SFTTrainer(\n",
        "        model=model,  # Base model to fine-tune\n",
        "        train_dataset=train_dataset,  # Training dataset\n",
        "        eval_dataset=eval_dataset,  # Validation dataset for evaluation\n",
        "        peft_config=lora_config,  # LoRA/PEFT configuration\n",
        "        args=train_config,  # Training arguments and hyperparameters\n",
        "    )\n",
        "\n",
        "    print(\"✓ Trainer created\")\n",
        "\n",
        "    return trainer\n",
        "\n",
        "\n",
        "def train_model(trainer, hub_model_name: str):\n",
        "    \"\"\"\n",
        "    Train the model and push to Hugging Face Hub.\n",
        "\n",
        "    Args:\n",
        "        trainer: SFTTrainer instance\n",
        "        hub_model_name: Hugging Face Hub model name\n",
        "    \"\"\"\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Starting Training\")\n",
        "    print(\"=\" * 60)\n",
        "    print(\"This may take a while depending on your hardware and dataset size...\")\n",
        "    print(\"The model will be automatically saved to Hugging Face Hub during training.\")\n",
        "    print()\n",
        "\n",
        "    # Start training\n",
        "    trainer.train()\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Training Complete!\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Push final model to hub\n",
        "    print(f\"\\nPushing final model to Hugging Face Hub: {hub_model_name}\")\n",
        "    trainer.model.push_to_hub(hub_model_name, private=True)\n",
        "    print(f\"✓ Model saved to: https://huggingface.co/{hub_model_name}\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    \"\"\"\n",
        "    Main function to orchestrate the fine-tuning process.\n",
        "    \"\"\"\n",
        "    print(\"=\" * 60)\n",
        "    print(\"Fine-Tuning Model with QLoRA\")\n",
        "    print(\"=\" * 60)\n",
        "    print(f\"Model: {BASE_MODEL}\")\n",
        "    print(f\"Mode: {'LITE' if LITE_MODE else 'FULL'}\")\n",
        "    print(f\"Run name: {RUN_NAME}\")\n",
        "    print(f\"Hub model: {HUB_MODEL_NAME}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "    # Step 1: Authenticate with Hugging Face and W&B\n",
        "    load_environment()\n",
        "\n",
        "    # Step 2: Initialize W&B if enabled\n",
        "    if LOG_TO_WANDB and WANDB_AVAILABLE:\n",
        "        wandb.init(project=PROJECT_NAME, name=RUN_NAME)\n",
        "        print(\"✓ Weights & Biases initialized\")\n",
        "\n",
        "    # Step 3: Load dataset\n",
        "    train, val, test = load_dataset_from_hub(\n",
        "        lite_mode=LITE_MODE,\n",
        "        username=DATA_USER,\n",
        "        val_size=VAL_SIZE\n",
        "    )\n",
        "\n",
        "    # Step 4: Detect compute dtype based on GPU capabilities\n",
        "    use_bf16, compute_dtype = detect_compute_dtype()\n",
        "\n",
        "    # Step 5: Create quantization configuration\n",
        "    quant_config = create_quantization_config(\n",
        "        use_4bit=QUANT_4_BIT,\n",
        "        compute_dtype=compute_dtype\n",
        "    )\n",
        "\n",
        "    # Step 6: Load base model with quantization\n",
        "    tokenizer, base_model = load_base_model(BASE_MODEL, quant_config)\n",
        "\n",
        "    # Step 7: Create LoRA configuration\n",
        "    lora_config = create_lora_config(\n",
        "        r=LORA_R,\n",
        "        alpha=LORA_ALPHA,\n",
        "        dropout=LORA_DROPOUT,\n",
        "        target_modules=TARGET_MODULES,\n",
        "        task_type=\"CAUSAL_LM\"\n",
        "    )\n",
        "\n",
        "    # Step 8: Create training configuration\n",
        "    train_config = create_training_config(\n",
        "        output_dir=PROJECT_RUN_NAME,\n",
        "        num_epochs=EPOCHS,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        learning_rate=LEARNING_RATE,\n",
        "        warmup_ratio=WARMUP_RATIO,\n",
        "        lr_scheduler_type=LR_SCHEDULER_TYPE,\n",
        "        weight_decay=WEIGHT_DECAY,\n",
        "        optimizer=OPTIMIZER,\n",
        "        max_sequence_length=MAX_SEQUENCE_LENGTH,\n",
        "        gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "        save_steps=SAVE_STEPS,\n",
        "        logging_steps=LOG_STEPS,\n",
        "        use_bf16=use_bf16,\n",
        "        hub_model_id=HUB_MODEL_NAME,\n",
        "        run_name=RUN_NAME,\n",
        "        log_to_wandb=LOG_TO_WANDB and WANDB_AVAILABLE\n",
        "    )\n",
        "\n",
        "    # Step 9: Create trainer\n",
        "    trainer = create_trainer(\n",
        "        model=base_model,\n",
        "        tokenizer=tokenizer,\n",
        "        train_dataset=train,\n",
        "        eval_dataset=val,\n",
        "        lora_config=lora_config,\n",
        "        train_config=train_config\n",
        "    )\n",
        "\n",
        "    # Step 10: Train model\n",
        "    train_model(trainer, HUB_MODEL_NAME)\n",
        "\n",
        "    # Step 11: Finish W&B run\n",
        "    if LOG_TO_WANDB and WANDB_AVAILABLE:\n",
        "        wandb.finish()\n",
        "        print(\"✓ Weights & Biases run finished\")\n",
        "\n",
        "    print(\"\\n\" + \"=\" * 60)\n",
        "    print(\"Fine-tuning complete!\")\n",
        "    print(f\"Model available at: https://huggingface.co/{HUB_MODEL_NAME}\")\n",
        "    print(\"=\" * 60)\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    # Set random seed for reproducibility\n",
        "    set_seed(42)\n",
        "\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01f38dabdc4b4735b729b2584ab01b9b": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "02507b8dee1d4437aa10f398711892cf": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "11cd72dab74e4fe1b1ff4b86012f9226": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6ee55e20253442ba80f42e33be413df8",
            "placeholder": "​",
            "style": "IPY_MODEL_5a9e9a4ba75541ccb4529c0ee0802515",
            "value": "Loading checkpoint shards: 100%"
          }
        },
        "26495c9de0d448fba45034c2438d1305": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2e4f37af0f3640cbac6c9f2a23971aca": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_11cd72dab74e4fe1b1ff4b86012f9226",
              "IPY_MODEL_77029cbffa9245df874a385becd30415",
              "IPY_MODEL_ceb8c838fea54b1987ddc0a74999ffcd"
            ],
            "layout": "IPY_MODEL_26495c9de0d448fba45034c2438d1305"
          }
        },
        "55a3029adaa0470a8d1e17d21fb6bd66": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5756ba5d09714ffeaa284420f4e963c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5a9e9a4ba75541ccb4529c0ee0802515": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6ee55e20253442ba80f42e33be413df8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "77029cbffa9245df874a385becd30415": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01f38dabdc4b4735b729b2584ab01b9b",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_02507b8dee1d4437aa10f398711892cf",
            "value": 2
          }
        },
        "ceb8c838fea54b1987ddc0a74999ffcd": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_55a3029adaa0470a8d1e17d21fb6bd66",
            "placeholder": "​",
            "style": "IPY_MODEL_5756ba5d09714ffeaa284420f4e963c1",
            "value": " 2/2 [00:40&lt;00:00, 18.32s/it]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
